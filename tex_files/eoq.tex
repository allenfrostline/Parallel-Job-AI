\documentclass{article}
\title{$Q$ learning}
\author{N.D. Van Foreest}
\usepackage{standardStyle}

\begin{document}
\maketitle

\tableofcontents

\section{Intro}
\label{sec:intro}



I want to apply $Q$ learning to the EOQ model. First I want to get
the tabular $Q$ learning algorithm to work.  For this,  I copy from
Bertsekas, Vol 2, Section 6.4 on $Q$ learning.  Then I'll apply it to the EOQ model. Before diving into the $Q$ learning algorithm, I'll discuss a few basic inventory models. 

\section{Inventory control models}
\label{sec:invent-contr-models}

The first distinction to be made is continuous-time versus periodic-time inventory models. Since we will simulate (with recursions) inventory systems, we can restrict our attention to periodic-time inventory models. 

For the Economic Order Quantity (EOQ) model, we assume that demand $D$ is deterministic and constant, e.g. 1 unit per period. (Strictly speaking, the EOQ is a continuous-time model, but we describe it here in terms of periodic-time.) There is an order cost $A$, holding cost $h$ per unit inventory per unit time. Replenishments are delivered immediately and and any replenishment size is allowed. Based on this cost model, it is optimal to order when the inventory hits $0$ and to order $Q=\sqrt{2AD/h}$

The $(s,S)$ model deals with random demand. When the inventory hits the set $(-\infty, s]$, a replenishment is issued such that after the receipt of the replenishment, the inventory becomes $S$. 
The level $s$ is known as the re-order point and  $S$ as the order-up-to level. Note that the inventory level can become less than $s$, thus the order size need \emph{not}  to equal to $S-s$. 
Note that for deterministic and constant demand, the $(s,S)$ model reduces to the EOQ with $s=0$ and $S=Q$. 

For the basestock model we assume that the reorder cost $A=0$ (or negligible). In that case, there is no reason to replenish in batch sizes larger than $1$. Thus, we set $S=s+1$. 

Another common model is to fix the order size $Q$ (contrary to the $(s,S)$ policy) and trigger a replenishment when the reorder set $(-\infty, r]$ is hit. This is the $(Q,r)$ model. When demand is deterministic, the $(Q,r)$ model becomes the EOQ model with $r=0$. 

Note that we assume that the leadtimes $L$, i.e., the time between issuing a replenishment and receiving it, are $0$.  In the cases we will consider below, the leadtime $L=0$. 

Below we concentrate first on the EOQ model, as this is the simplest possible, but non-trivial, model. Then we extend to stochastic demand. It is known that the for this case, the $(s,S)$-policy is optimal when holding and backlogging cost are convex in the inventory level. Thus, it is interesting to see whether $Q$-learning can figure out the optimal $s$ and $S$. Perhaps this is not so easy: I suspect that the cost function is not very sensitive to $s$ and $S$, (as it the case of the EOQ model). It is easy to check this with a numerical analysis (I have implemented an algorithm in python to compute the optimal $s$ and $S$.)

\section{Q factors}
\label{sec:q-factors}


Define the $Q$-factors
as
\begin{equation*}
  Q^*(i,u) = \sum_{j} p_{ij}(u)\left[g(i,u, j) + \alpha J^*(j)\right],
\end{equation*}
where $g(i, u, j)$ is the reward of taking action $u$ in state $i$ and end up in state $j$, and $J^*(j)$ is the value of continuing in state $j$. 
With this expression for $Q^*(i,u)$ the dynamic programming (Bellman) equations take the form
\begin{equation*}
  J^*(i) 
= \max_{u\in U(i)}\left\{\sum_{j} p_{ij}(u)\left[g(i,u, j) + \alpha J^*(j)\right]\right\}
= \max_{u\in U(i)} Q^*(i,u).
\end{equation*}
From this we can get an optimality equation for  the $Q$ factors:
\begin{align*}
  J^*(j) &= \max_{u'\in U(j)} Q^*(j,u') \iff \\
\alpha J^*(j) &=   \alpha \max_{u'\in U(j)} Q^*(j,u') \iff \\
  \sum_{j} p_{ij}(u) \alpha J^*(j) &=   \sum_{j} p_{ij}(u) \alpha \max_{u'\in U(j)} Q^*(j,u') \iff \\
  \sum_{j} p_{ij}(u)\left[g(i,u,j) +  \alpha J^*(j)\right] &=   \sum_{j} p_{ij}(u) \left[g(i,u,j) + \alpha \max_{u'\in U(j)} Q^*(j,u')\right] \iff \\
  Q^*(i,u) &=   \sum_{j} p_{ij}(u) \left[g(i,u,j) + \alpha \max_{u'\in U(j)} Q^*(j,u')\right].
\end{align*}

We can use this to compute $Q^*$ by value-iteration.  Start with
$Q(i,u)=0$ (or some other set of random values), and iterate according to
\begin{equation}\label{eq:1}
  Q(i,u) :=   \sum_{j} p_{ij}(u) \left[g(i,u,j) + \alpha \max_{u'\in U(j)} Q(j,u')\right].
\end{equation}

Now we can do two things for $Q$-learning.  The first is to  simulate $j$  and $g(i,u,j)$ from the
pair $(i,u)$ (see below how to do this), and take $\alpha=1$ to obtain a total cost during a finite horizon, and replace the above expectation by the update rule
\begin{equation*}
  Q(i,u)  = g(i,u,j) + \max_{u'\in U(j)} Q(j,u').
\end{equation*}
To explore and exploit and we can choose to use
$u' = \argmax Q^*(j,u')$ with, for instance, probability $0.9$ and
otherwise take $u'$ uniform in $U(j)$.

The second thing is to introduce a damping factor~$\gamma\in[0,1]$ in~(\ref{eq:1}) so that we obtain 
\begin{equation*}
  \begin{split}
  Q(i,u) 
&=  (1-\gamma)Q(i,u) + \gamma \sum_{j} p_{ij}(u) \left[g(i,u,j) + \alpha \max_{u'\in U(j)} Q(j,u')\right] \\
&=  Q(i,u) + \gamma \left(\sum_{j} p_{ij}(u) \left[g(i,u,j) + \alpha \max_{u'\in U(j)} Q(j,u')\right] - Q(i,u)\right).
  \end{split}
\end{equation*}
Now we simulate $j$ and $g(i,u,j)$ from the
pair $(i,u)$ and replace the above expectation by the update rule
\begin{equation}\label{eq:3}
  \begin{split}
  Q(i,u) 
&:=  Q(i,u) + \gamma \left(g(i,u,j) + \alpha \max_{u'\in U(j)} Q(j,u')- Q(i,u)\right).
  \end{split}
\end{equation}
To explore and exploit  we can use the same trick as before.

\begin{remark}
  To simulate $j$ from the pair $(i,u)$,  we can use the transition probabilities $p_{ij}(u)$ as follows. Let $J$ be the next state (random variable) from state $(i,u)$. Then set $\P{J=j}_{(i,u)} = p_{ij}(u)$.  Now that we have generated the next state $j$ from $(i,u)$ we can compute the reward $g(i,u,j)$. 
\end{remark}


\section{Using Heuristics To initialize the $Q$ values}
\label{sec:using-heuristics-as}

Suppose we ask a human to solve the optimization problem at hand. Of course we don't expect that the human player obtains an optimal solution, but we like to use the actions of the human to give the optimizer a head start. There must  be a number of ways to achieve this. So, lets assume the user played the game a few times. (I also assume for ease that the player took the same actions in the same state. This is not true of course, but we can tackle this issue later.). Now we have a trace $T = ((i_1,u_1), (i_2,u_2),\ldots)$ (so the user chose action $u_1$ in state $i_1$, and so on.). In other words, a trace is the set of states and actions that occured during the game played by the human.  (Here a trace is an ordered set, i.e., a tuple, of how the game proceeded.)

Once we have a trace, or a few traces, we need to initialze the $Q$ values to start the learning algorithm. One simple way is like this: 
\begin{equation*}
  Q(i,u) = 
  \begin{cases}
    \max_j g(i,u,j),  &\text{ if } (i,u) \in T, \\
    0  &\text{ if } (i,u) \not \in T.
  \end{cases}
\end{equation*}
Assume here that $g(i,u,j)\geq 0$ for all $i, u, j$.  For inventory or scheduling problems with infinite horizon (so there is no $\partial$) but a reward per step (acceptance of a job), this initialization might be ok. 

This choice is not ok if $g=0$ for many $i,u,j$, and when the player only gets a reward at the end of the game, like in chess or go. So, assuming that the player plays the game until it finishes,  then let $(i_n, u_n)$ be the finishing  state of the game, and $n$ the finishing time. As the game stopped here, it must be that $g(i_n, u_n, \partial)>0$, where $\partial$ is the coffin(finish) state). For instance, $\partial = \text{ mate }$ for chess. Then set 
\begin{equation*}
  Q(i,u) = 
  \begin{cases}
    g(i_n,u_n,\partial),  &\text{ if } (i,u) \in T, \\
    0  &\text{ if } (i,u) \not \in T.
  \end{cases}
\end{equation*}





\begin{enumerate}
\item How to handle different choices of the player in the same state $(i,u)$? 
\item There is some explanation in Sutton's book on how to use traces to update the $Q$-values. I forgot where precisely, but I suspect it is in chapter 4.
\item Isn't there anything in Bertsekas, dynamic progamming volume 2,  on this too? I don't think so, but it might be interesting to check.
\end{enumerate}


\section{An Application to an inventory system that satisfies the EOQ assumptions}


Standard EOQ model under periodic review such that the review epochs are synchronized with the demand arrivals.. Orders are placed at the
start of a period and arrive right away, i.e., immediate replenishments.

\begin{align*}
  D&= \quad \text{Demand per period},\\
  h&= \quad \text{Holding cost due at the end of the period},\\
  p&= \quad \text{Selling price per unit},\\
  K&= \quad \text{Ordering cost per order},\\
\end{align*}

Let $I_n$ be the inventory at the end of the period and $Q_n$ the order size. Assuming that replenishments arrive at the start of the period, $I_n$ satisfies the recursion
\begin{align*}
S_n&=\min\{I_{n-1} + Q_n, D_n\}, \\
I_n&=\max\{I_{n-1}+Q_n - s_n, I_{\max}\}, 
\end{align*}
where $S_n$ is the sales during period $n$.
Thus, in the notation of Section~\ref{sec:q-factors},  the inventory $I_{n-1}=i$, the action  
$u_n=Q_n$ and  $j=I_n$. With this, it is easy to compute $p_{ij}(u)$. 

The end-of-period reward is given by
\begin{equation*}
 R_n = p s_n - h I_n - K \1{Q_n > 0}.
\end{equation*}
Thus, $g(i,u, j) = R_n$.

The state space is  $I_n \in \{0, 1, \ldots, I_{\max}\}$, the action space is $\{0, \ldots, Q_{\max}\}$. 

In the learning procedure we want to reduce the exploration rate while the number of episodes progresses. We take
\begin{equation*}
  \beta(i) = 0.8 + 0.2\frac i N,
\end{equation*}
if $i$ is the current episode of $N$ episodes in total. I also tried the rule
\begin{equation*}
  \beta(i) = 0.8 
\end{equation*}
for all $i$. This also gave good results.


\begin{figure}[tb]
  \centering
  \begin{tabular}[h]{cc}
%\input{eoq_learning_scenario_1}\\
  \end{tabular}
  \caption{piet}
\end{figure}


% \lstinputlisting[language=Python]{eoq.py}

\subsection{First test}

TBD: Include results.

\subsection{(s,S) -policies}
\label{sec:s-s-policies}

TBD: Include results.


\subsection{Possible Next Steps}
\label{sec:next-steps}


\begin{itemize}
\item One way that must give much better results is to use~(\ref{eq:1}) for the iteration, rather than use~(\ref{eq:3}). That must be a major improvement. Of course, this doesn't work for large problems, but is of interest to see how these two compare.
\item (S,s) ?
\item (Q,r) policy
\item use neural network. Check the ice/hole/example on how to do this. 
\end{itemize}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
